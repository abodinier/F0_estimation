{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import mirdata\n",
    "import soundfile\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from training import Trainer\n",
    "from model import PaperModel\n",
    "from data import SalienceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/Users/alexandre/mir_datasets/medleydb_pitch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"first_attempt\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "EXP_FOLDER = Path(EXP_NAME + \"_\" + TIMESTAMP)\n",
    "EXP_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "SUMMARY_WRITER = SummaryWriter(str(EXP_FOLDER/EXP_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-2\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "INPUT_DIM = 5\n",
    "DEVICE = \"cpu\"\n",
    "N_EPOCHS = 100\n",
    "\n",
    "HP = {\n",
    "    \"LR\": 1e-3,\n",
    "    \"WEIGHT_DECAY\": 1e-4,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"INPUT_DIM\": 5,\n",
    "    \"DEVICE\": \"cpu\",\n",
    "    \"N_EPOCHS\": 100,\n",
    "}\n",
    "with open(EXP_FOLDER/\"hyper_parameters.json\", \"w\") as f:\n",
    "    json.dump(HP, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PaperModel()\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(lr=LR, params=model.parameters(), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_data = SalienceDataset(DATA_DIR/\"train\", ratio=0.1)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_data = SalienceDataset(DATA_DIR/\"validation\", ratio=0.01)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXP_FOLDER/\"model.p\", 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_data=train_loader,\n",
    "    val_data=val_loader,\n",
    "    loss_cls=loss,\n",
    "    optimizer=optim,\n",
    "    device=DEVICE,\n",
    "    summary_writer=SUMMARY_WRITER,\n",
    "    ckp_path=EXP_FOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "trainer.train(N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./first_attempt_19082022_143140/model.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./first_attempt_19082022_143140/ckp.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperModel(\n",
       "  (conv1): Conv2d(5, 128, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(64, 8, kernel_size=(3, 70), stride=(1, 1), padding=same)\n",
       "  (bn5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 22050\n",
    "BINS_PER_SEMITONE = 3\n",
    "N_OCTAVES = 6\n",
    "FMIN = 32.7\n",
    "BINS_PER_OCTAVE = 12 * BINS_PER_SEMITONE\n",
    "N_BINS = N_OCTAVES * BINS_PER_OCTAVE\n",
    "HOP_LENGTH = 512  # 23 ms hop\n",
    "N_TIME_FRAMES = 50  # 1.16 seconds\n",
    "N_AUDIO_SAMPLES = HOP_LENGTH * N_TIME_FRAMES\n",
    "N_EXAMPLES_PER_TRACK = 100\n",
    "\n",
    "CQT_FREQUENCIES = librosa.cqt_frequencies(N_BINS, FMIN, BINS_PER_OCTAVE)\n",
    "\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    y, _ = librosa.load(audio_path, sr=TARGET_SR, mono=True)\n",
    "    return y\n",
    "\n",
    "\n",
    "def compute_hcqt(audio):\n",
    "    cqt = librosa.cqt(\n",
    "        audio,\n",
    "        sr=TARGET_SR,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        fmin=FMIN,\n",
    "        n_bins=N_BINS,\n",
    "        bins_per_octave=BINS_PER_OCTAVE,\n",
    "    )\n",
    "    cqt = (1.0 / 80.0) * librosa.amplitude_to_db(np.abs(cqt), ref=1.0) + 1.0\n",
    "\n",
    "    hcqt = librosa.interp_harmonics(cqt, CQT_FREQUENCIES, [0.5, 1, 2, 3, 4])  # there are 5 harmonics ranges this is the dimension 0\n",
    "    return hcqt.transpose([0, 2, 1])\n",
    "\n",
    "\n",
    "def get_cqt_times(n_bins):\n",
    "    return librosa.frames_to_time(np.arange(n_bins), sr=TARGET_SR, hop_length=HOP_LENGTH)\n",
    "\n",
    "mirdata.initialize(\"vocadito\")\n",
    "\n",
    "def sonify_outputs(save_path, times, freqs, voicing):\n",
    "    \"\"\"Sonify the model outputs\n",
    "\n",
    "    Args:\n",
    "        save_path (str): path to save the audio file\n",
    "        times (np.ndarray): time stamps (in seconds)\n",
    "        freqs (np.ndarray): f0 values (in Hz)\n",
    "        voicing (np.ndarray): voicing (between 0 and 1)\n",
    "    \"\"\"\n",
    "    y = mir_eval.sonify.pitch_contour(times, freqs, 8000, amplitudes=voicing)\n",
    "    soundfile.write(save_path, y, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "audio = load_audio(\"./A Classic Education - NightOwl.stem.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, audio_path):\n",
    "    \"\"\"Run model inference on a full-length audio file\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): pytorch model\n",
    "        audio_path (str): path to audio file to run inference on\n",
    "\n",
    "    Returns:\n",
    "        mirdata.annotations.F0Data: f0 data object, containing predicted f0 data\n",
    "    \"\"\"\n",
    "    model.eval()  # put the model in evaluation mode\n",
    "\n",
    "    # load audio (at the fixed sample rate)\n",
    "    y = load_audio(audio_path)\n",
    "\n",
    "    # compute input features\n",
    "    hcqt = compute_hcqt(y)\n",
    "    n_times = hcqt.shape[1]\n",
    "\n",
    "    # the number of frames to run inference on at a time\n",
    "    slice_size = 200\n",
    "    outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # loop over the full time range\n",
    "        for i in np.arange(0, n_times, step=slice_size):\n",
    "            hcqt_tensor = torch.tensor(hcqt[np.newaxis, :, i : i + slice_size, :]).float()\n",
    "            predicted_salience = model(hcqt_tensor)\n",
    "            predicted_salience = nn.Sigmoid()(predicted_salience).detach()\n",
    "            outputs.append(predicted_salience)\n",
    "\n",
    "    # concatenate the outputs\n",
    "    # NOTE: this is not the best approach! This will have boundary effects\n",
    "    # every slice_size frames. To improve this, use e.g. overlap add\n",
    "    unwrapped_prediction = np.hstack(outputs)[0, :n_times, :, 0].astype(float)\n",
    "\n",
    "    # decode the output predictions into a single time series using viterbi decoding\n",
    "    transition_matrix = librosa.sequence.transition_local(len(CQT_FREQUENCIES), 5)\n",
    "    predicted_pitch_idx = librosa.sequence.viterbi(unwrapped_prediction.T, transition_matrix)\n",
    "\n",
    "    # compute f0 and amplitudes using predicted indexes\n",
    "    predicted_pitch = np.array([CQT_FREQUENCIES[f] for f in predicted_pitch_idx])\n",
    "    predicted_salience = np.array(\n",
    "        [unwrapped_prediction[i, f] for i, f in enumerate(predicted_pitch_idx)]\n",
    "    )\n",
    "    times = get_cqt_times(n_times)\n",
    "    return mirdata.annotations.F0Data(\n",
    "        times, \"s\", predicted_pitch, \"hz\", predicted_salience, \"likelihood\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "pred = run_inference(model, \"./A Classic Education - NightOwl.stem.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_times, est_freqs, est_voicing = pred.to_mir_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# sonify the estimates\n",
    "sonify_outputs(\n",
    "    os.path.join(\"./\", f\"test_f0est.wav\"),\n",
    "    est_times,\n",
    "    est_freqs,\n",
    "    est_voicing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, audio_path):\n",
    "    \"\"\"Run model inference on a full-length audio file\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): pytorch model\n",
    "        audio_path (str): path to audio file to run inference on\n",
    "\n",
    "    Returns:\n",
    "        mirdata.annotations.F0Data: f0 data object, containing predicted f0 data\n",
    "    \"\"\"\n",
    "    model.eval()  # put the model in evaluation mode\n",
    "\n",
    "    # load audio (at the fixed sample rate)\n",
    "    y = load_audio(audio_path)\n",
    "\n",
    "    # compute input features\n",
    "    hcqt = compute_hcqt(y)\n",
    "    n_times = hcqt.shape[1]\n",
    "\n",
    "    # the number of frames to run inference on at a time\n",
    "    slice_size = 200\n",
    "    outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # loop over the full time range\n",
    "        for i in np.arange(0, n_times, step=slice_size):\n",
    "            hcqt_tensor = torch.tensor(hcqt[np.newaxis, :, i : i + slice_size, :]).float()\n",
    "            predicted_salience = model(hcqt_tensor)\n",
    "            predicted_salience = nn.Sigmoid()(predicted_salience).detach()\n",
    "            outputs.append(predicted_salience)\n",
    "\n",
    "    # concatenate the outputs\n",
    "    # NOTE: this is not the best approach! This will have boundary effects\n",
    "    # every slice_size frames. To improve this, use e.g. overlap add\n",
    "    unwrapped_prediction = np.hstack(outputs)[0, :n_times, :, 0].astype(float)\n",
    "\n",
    "    # decode the output predictions into a single time series using viterbi decoding\n",
    "    transition_matrix = librosa.sequence.transition_local(len(CQT_FREQUENCIES), 5)\n",
    "    predicted_pitch_idx = librosa.sequence.viterbi(unwrapped_prediction.T, transition_matrix)\n",
    "\n",
    "    # compute f0 and amplitudes using predicted indexes\n",
    "    predicted_pitch = np.array([CQT_FREQUENCIES[f] for f in predicted_pitch_idx])\n",
    "    predicted_salience = np.array(\n",
    "        [unwrapped_prediction[i, f] for i, f in enumerate(predicted_pitch_idx)]\n",
    "    )\n",
    "    times = get_cqt_times(n_times)\n",
    "    return mirdata.annotations.F0Data(\n",
    "        times, \"s\", predicted_pitch, \"hz\", predicted_salience, \"likelihood\"\n",
    "    )\n",
    "\n",
    "\n",
    "def sonify_outputs(save_path, times, freqs, voicing):\n",
    "    \"\"\"Sonify the model outputs\n",
    "\n",
    "    Args:\n",
    "        save_path (str): path to save the audio file\n",
    "        times (np.ndarray): time stamps (in seconds)\n",
    "        freqs (np.ndarray): f0 values (in Hz)\n",
    "        voicing (np.ndarray): voicing (between 0 and 1)\n",
    "    \"\"\"\n",
    "    y = mir_eval.sonify.pitch_contour(times, freqs, 8000, amplitudes=voicing)\n",
    "    soundfile.write(save_path, y, 8000)\n",
    "\n",
    "\n",
    "def evaluate(model, track_path, sonification_dir):\n",
    "    \"\"\"Run evaluation on vocadito\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): pytorch model\n",
    "        sonification_dir (str): path to save sonifications\n",
    "\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    medley_db = mirdata.initialize(\"medleydb_pitch\")\n",
    "    # loop over the tracks in medley_db\n",
    "    \n",
    "    estimated_f0 = run_inference(model, track_path)\n",
    "\n",
    "    # get the estimated f0 in mir_eval format\n",
    "    est_times, est_freqs, est_voicing = estimated_f0.to_mir_eval()\n",
    "\n",
    "    # sonify the estimates\n",
    "    sonify_outputs(\n",
    "        os.path.join(sonification_dir, f\"test_f0est.wav\"),\n",
    "        est_times,\n",
    "        est_freqs,\n",
    "        est_voicing,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "medley_db = mirdata.initialize(\"medleydb_pitch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for track_id in medley_db.track_ids:\n",
    "    track_path = medley_db.track(track_id).audio_path\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
